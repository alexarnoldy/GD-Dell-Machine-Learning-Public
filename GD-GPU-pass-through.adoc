sudo zypper -n in libvirt kvm virt-manager
echo set -o vi >> ~/.bashrc && . ~/.bashrc

sudo systemctl start libvirtd.service

sudo zypper -n install xorg-x11-Xvnc

sudo usermod -aG libvirt geeko

vncserver :3


### For GPU pass-through to VM

* Ensure all of the required tools are installed:
** `sudo zypper --non-interactive install  libvirt libvirt-client libvirt-daemon virt-manager virt-install virt-viewer qemu qemu-kvm qemu-ovmf-x86_64 qemu-tools`

* Add iommu=pt intel_iommu=on  to the GRUB_CMDLINE_LINUX_DEFAULT line in /etc/default/grub
* `sudo  grub2-mkconfig -o /boot/grub2/grub.cfg`


* create /etc/modules-load.d/vfio_pci.conf
----
# load vfio_pci module at boot time

vfio_pci
----

* Reboot the node
* After the reboot, ensure IOMMU is enabled `sudo dmesg | grep "IOMMU enabled"`

* Run `sudo lspci -nn | less`
** Search for Nvidia
* Capture the bus ID in the first column (i.e. 3b:00.0)
* Capture the VENDOR_ID and PRODUCT_ID, and the bus ID. Bus ID will be at the beginning of the line, VENDOR_ID and PRODUCT_ID will be near the end of line, in brackets and separated by a colon (i.e. [10de:1eb8] )

* Run `sudo lspci -nns <bus ID>`

* Set the variable: BUS_ID=""
* Run `ls -l /sys/kernel/iommu_groups/*/devices/* | grep $BUS_ID`
** Capture the group number, i.e. group 33 for `/sys/kernel/iommu_groups/33/devices/0000:3b:00.0`

* Create the file `/etc/modprobe.d/gpu-passthrough.conf`
** Populate with `options vfio-pci ids=` followed by the VENDOR_ID and PRODUCT_ID

* Create the file `/etc/dracut.conf.d/gpu-passthrough.conf`
** Populate with `add_drivers+=” pci_stub vfio vfio_iommu_type1 vfio_pci vfio_virqfd kvm kvm_intel “`

* Rebuild initrd: `sudo dracut -f /boot/initrd $(uname -r)`

* Reboot the node
* After the reboot, verify that the GPU is managed by the kernel driver vfio-pci driver
** Run `sudo lspci -k | less`
*** Search for Nvidia

* The GPU is now ready to pass through to a VM

* Use virt-manager to create a VM to use the GPU, or add the GPU by it's bus ID to an existing VM
** Find it by the name NVIDIA or by the bus ID

* After the VM boots, verify the GPU is present: `sudo lspci -nn | grep -i nvidia`




### Enabling nvidia-container-runtime

.Start on the GPU equiped Worker Node

https://devblogs.nvidia.com/gpu-containers-runtime/
The latest NVIDIA driver. Use the package manager to install the cuda-drivers package
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#sles-installation

### From https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions
* Also: https://fenix.tecnico.ulisboa.pt/downloadFile/563568428738334/CUDA_Installation_Guide_Linux.pdf

* `sudo lspci | grep -i nvidia`
** Check against: https://developer.nvidia.com/cuda-gpus to ensure the GPU is CUDA compatible

* The CUDA Development Tools are only supported on some specific distributions of Linux. These are listed in the CUDA Toolkit release notes.
* Determine the Linux distribution and release: `uname -m && cat /etc/*release`

////
Instructions say gcc is only required for development but I haven't tested a deployment without it
* `gcc --version`
** If gcc is not installed: `sudo zypper --non-interactive install gcc`
////

.Install the appropriate kernel header files for the kernel version
* `uname -r`
** Output is in the form of <version>-<variant>, i.e. 4.12.14-197.26-default
* `sudo zypper --non-interactive install kernel-default-devel=4.12.14-197.26`
** Format is sudo zypper --non-interactive install kernel-<variant>-devel=<version>

* Install the Cuda toolkit:
----
sudo zypper addrepo http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/cuda-sles15.repo
sudo SUSEConnect --product PackageHub/15.1/x86_64
sudo SUSEConnect --product sle-module-desktop-applications/15.1/x86_64
sudo zypper refresh	# May require "trust always" the "package signing key"
sudo zypper --non-interactive install cuda
----
                                                                                                                                                  
////
Noticed that it installed the 10-2 version of every cuda package available except for cuda-compat-10-2
////

* Verify which packages and versions of the Cuda toolkit were installed `sudo zypper search cuda`

* When the driver is correctly loaded it will show the version in: `cat /proc/driver/nvidia/version`
** If the driver hasn't loaded, reboot the node and check again

### From https://hackweek.suse.com/18/projects/architecting-a-machine-learning-project-with-suse-caasp

* "Note I am not installing nvidia-container-runtime, but only the hook. That is because we will use cri-o and not docker. For cri-o we don't need to install the nvidia-container-runtime."



### From https://github.com/jordimassaguerpla/SUSE_hackweek_18/blob/master/01-How_to_setup_SUSE_CaaSP_kubernetes_crio_GPU.md

* Check that you can access the GPU:
----
sudo usermod -G video -a geeko
sudo usermod -G video -a root
sudo su - geeko
nvidia-smi
----
** Should get an output that contains:
----
NVIDIA-SMI XXX.YY Driver Version: XXX.YY CUDA Version: XX.Y
. . . .
No running processes found
----

* Install the Nvidia libnvidia-container:
----
wget https://github.com/NVIDIA/libnvidia-container/releases/download/v1.0.0/libnvidia-container_1.0.0_x86_64.tar.xz
tar xJf libnvidia-container_1.0.0_x86_64.tar.xz
sudo cp libnvidia-container_1.0.0/usr/local/bin/nvidia-container-cli /usr/bin
sudo cp libnvidia-container_1.0.0/usr/local/lib/libnvidia-container.so* /usr/lib64
----
* Verify functionality of the nvidia-container-cli utility: `nvidia-container-cli info`
** Should get an output that contains:
----
NVRM version:   XXX.YY                                                          
CUDA version:   XX.Y  
Model:		X
Brand:		Y
----

### Install nvidia-container-toolkit on any nodes equiped with GPUs

.Start on the CaaS Platform Administrative Workstation
* Download the required package via a CentOS container:
----
sudo zypper --non-interactive install podman
sudo podman run --rm -ti -v$PWD:/var/tmp centos:7
DIST=$(. /etc/os-release; echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$DIST/nvidia-container-runtime.repo |    tee /etc/yum.repos.d/nvidia-container-runtime.repo
yum install --downloadonly nvidia-container-runtime-hook	# May have answer "y" to accept Nvidia's GPG key
cp /var/cache/yum/x86_64/7/nvidia-container-runtime/packages/nvidia-container-toolkit-1.0.5-2.x86_64.rpm /var/tmp
exit
----

* Create the unrpm script from: https://github.com/openSUSE/obs-build/blob/master/unrpm
* Unpack the rpm: `bash unrpm nvidia-container-toolkit-1.0.5-2.x86_64.rpm`
* If running this on another node (i.e. the Administrative Workstation), SCP the files to the GPU Worker Node: 
** Set this variable to the FQDN of the GPU Worker Nodes `WORKER=""`
** SCP the files to the Worker Node:

////
----
ssh $WORKER sudo mkdir -p /etc/nvidia-container-runtime/
scp etc/nvidia-container-runtime/config.toml root@$WORKER:/etc/nvidia-container-runtime/config.toml 
scp usr/share/containers/oci/hooks.d/oci-nvidia-hook.json root@$WORKER:/usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
ssh $WORKER sudo mkdir -p /usr/libexec/oci/hooks.d/
scp usr/libexec/oci/hooks.d/oci-nvidia-hook root@$WORKER:/usr/libexec/oci/hooks.d/oci-nvidia-hook
ssh $WORKER sudo mkdir -p /usr/share/licenses/nvidia-container-toolkit-1.0.5/
scp usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE root@$WORKER:/usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE
----
////

----
scp -r etc/nvidia-container-runtime/config.toml $WORKER:~/etc/nvidia-container-runtime/config.toml
scp -r usr/share/containers/oci/hooks.d/oci-nvidia-hook.json $WORKER:~/usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
scp -r usr/libexec/oci/hooks.d/oci-nvidia-hook $WORKER:~/usr/libexec/oci/hooks.d/oci-nvidia-hook
scp -r usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE $WORKER:~/usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE
----

.Continue the process on the GPU Worker Node:

* Copy the Nvidia Container Toolkit into place:
----
sudo mkdir -p /etc/nvidia-container-runtime/
sudo mkdir -p /usr/libexec/oci/hooks.d/
sudo cp etc/nvidia-container-runtime/config.toml /etc/nvidia-container-runtime/config.toml
sudo cp usr/bin/nvidia-container-toolkit /usr/bin/nvidia-container-toolkit
sudo cp usr/share/containers/oci/hooks.d/oci-nvidia-hook.json /usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
sudo cp usr/libexec/oci/hooks.d/oci-nvidia-hook /usr/libexec/oci/hooks.d/oci-nvidia-hook
sudo cp usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE /usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE
----

* Edit  the /etc/nvidia-container-runtime/config.toml file to uncomment or insert the line: `user = "root:video"`

* Update the metadata of the Nvidia device files:
----
sudo chmod 0666 /dev/nvidia*
sudo chown root:video /dev/nvidia*
----

* (Optional) Test that a container can access the GPU: `sudo podman --rm run nvidia/cuda nvidia-smi`

.Install the Nvidia Kubernetes device plugin from the Administrative Workstation
* `kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml`

                                                                                                                                                  
// vim: set syntax=asciidoc:  
