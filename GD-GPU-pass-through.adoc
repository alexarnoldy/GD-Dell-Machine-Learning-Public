sudo zypper -n in libvirt kvm virt-manager
echo set -o vi >> ~/.bashrc && . ~/.bashrc

sudo systemctl start libvirtd.service

sudo zypper -n install xorg-x11-Xvnc

sudo usermod -aG libvirt geeko

vncserver :3



* Add iommu=pt intel_iommu=on  to the GRUB_CMDLINE_LINUX_DEFAULT line in /etc/default/grub
* `sudo  grub2-mkconfig -o /boot/grub2/grub.cfg`


* create /etc/modules-load.d/vfio_pci.conf
----
# load vfio_pci module at boot time

vfio_pci
----

* Reboot the node
* After the reboot, ensure IOMMU is enabled `sudo dmesg | grep "IOMMU enabled"`

* Run `sudo lspci -nn | less`
** Search for Nvidia
* Capture the bus ID in the first column (i.e. 3b:00.0)
* Capture the VENDOR_ID and PRODUCT_ID, and the bus ID. Bus ID will be at the beginning of the line, VENDOR_ID and PRODUCT_ID will be near the end of line, in brackets and separated by a colon (i.e. [10de:1eb8] )

* Run `sudo lspci -nns <bus ID>`

* Set the variable: BUS_ID=""
* Run `ls -l /sys/kernel/iommu_groups/*/devices/* | grep $BUS_ID`
** Capture the group number, i.e. group 33 for `/sys/kernel/iommu_groups/33/devices/0000:3b:00.0`

* Create the file `/etc/modprobe.d/gpu-passthrough.conf`
** Populate with `options vfio-pci ids=` followed by the VENDOR_ID and PRODUCT_ID

* Create the file `/etc/dracut.conf.d/gpu-passthrough.conf`
** Populate with `add_drivers+=” pci_stub vfio vfio_iommu_type1 vfio_pci vfio_virqfd kvm kvm_intel “`

* Rebuild initrd: `sudo dracut -f /boot/initrd $(uname -r)`

* Reboot the node
* After the reboot, verify that the GPU is managed by the kernel driver vfio-pci driver
** Run `sudo lspci -k | less`
*** Search for Nvidia

* The GPU is now ready to pass through to a VM

* Use virt-manager to create a VM to use the GPU, or add the GPU by it's bus ID to an existing VM
** Find it by the name NVIDIA or by the bus ID

* After the VM boots, verify the GPU is present: `sudo lspci -nn | grep -i nvidia`


* `sudo zypper -n install  libvirt libvirt-client libvirt-daemon virt-manager virt-install virt-viewer qemu qemu-kvm qemu-ovmf-x86_64 qemu-tools`


### Enabling nvidia-container-runtime

https://devblogs.nvidia.com/gpu-containers-runtime/
The latest NVIDIA driver. Use the package manager to install the cuda-drivers package
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#sles-installation

### From https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions
* Also: https://fenix.tecnico.ulisboa.pt/downloadFile/563568428738334/CUDA_Installation_Guide_Linux.pdf

* `lspci | grep -i nvidia`
** Check against: https://developer.nvidia.com/cuda-gpus to ensure the GPU is CUDA compatible

* `uname -m && cat /etc/*release`
* `gcc --version`
** `sudo zypper -n install gcc`

* `uname -r`
** Output is in the form of <version>-<variant>, i.e. 4.12.14-197.26-default
* `sudo zypper install kernel-default-devel=4.12.14-197.26`
** Format is sudo zypper install kernel-<variant>-devel=<version>

----
sudo zypper addrepo http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/cuda-sles15.repo
sudo SUSEConnect --product PackageHub/15.1/x86_64
sudo SUSEConnect --product sle-module-desktop-applications/15.1/x86_64
sudo zypper refresh
sudo zypper install -y cuda
----
                                                                                                                                                  
* Note that it installed the 10-2 version of every cuda package available except for cuda-compat-10-2

* When the driver is correctly loaded it will show the version in: `cat /proc/driver/nvidia/version`

### From https://hackweek.suse.com/18/projects/architecting-a-machine-learning-project-with-suse-caasp

* "Note I am not installing nvidia-container-runtime, but only the hook. That is because we will use cri-o and not docker. For cri-o we don't need to install the nvidia-container-runtime."



### From https://github.com/jordimassaguerpla/SUSE_hackweek_18/blob/master/01-How_to_setup_SUSE_CaaSP_kubernetes_crio_GPU.md

* `sudo usermod -G video -a geeko`
* `sudo usermod -G video -a root`
* `sudo su - geeko`
* `nvidia-smi`
** Should get an output that contains:
----
NVIDIA-SMI XXX.YY Driver Version: XXX.YY CUDA Version: XX.Y
. . . .
No running processes found
----

* `wget https://github.com/NVIDIA/libnvidia-container/releases/download/v1.0.0/libnvidia-container_1.0.0_x86_64.tar.xz`
* `tar xJf libnvidia-container_1.0.0_x86_64.tar.xz`
* `sudo cp libnvidia-container_1.0.0/usr/local/bin/nvidia-container-cli /usr/bin`
* `sudo cp libnvidia-container_1.0.0/usr/local/lib/libnvidia-container.so* /usr/lib64`
* `nvidia-container-cli info`
** Should get an output that contains:
----
NVRM version:   XXX.YY                                                          
CUDA version:   XX.Y  
Model:		X
Brand:		Y
----

* Process for installing nvidia-container-toolkit:
** Start on the CaaS Platform Admin Workstation
----
podman run -ti -v$PWD:/var/tmp centos:7
DIST=$(. /etc/os-release; echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$DIST/nvidia-container-runtime.repo |    tee /etc/yum.repos.d/nvidia-container-runtime.repo
yum install --downloadonly nvidia-container-runtime-hook
cp /var/cache/yum/x86_64/7/nvidia-container-runtime/packages/nvidia-container-toolkit-1.0.5-2.x86_64.rpm /var/tmp
exit
----

*** Create the unrpm script from: https://github.com/openSUSE/obs-build/blob/master/unrpm
*** Unpack the rpm: `bash unrpm nvidia-container-toolkit-1.0.5-2.x86_64.rpm`
*** SCP the files to the GPU Worker Node: 
**** Set this variable to the FQDN of the GPU Worker Nodes `WORKER=""`
**** Execute these commands:
----
ssh $WORKER sudo mkdir -p /etc/nvidia-container-runtime/
scp etc/nvidia-container-runtime/config.toml root@$WORKER:/etc/nvidia-container-runtime/config.toml 
scp usr/share/containers/oci/hooks.d/oci-nvidia-hook.json root@$WORKER:/usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
ssh $WORKER sudo mkdir -p /usr/libexec/oci/hooks.d/
scp usr/libexec/oci/hooks.d/oci-nvidia-hook root@$WORKER:/usr/libexec/oci/hooks.d/oci-nvidia-hook
----

** Continue the process on the GPU Worker Node:
*** Edit  the /etc/nvidia-container-runtime/config.toml file to uncomment or insert the line: `user = "root:video"`






* `sudo chmod 0666 /dev/nvidia*`
* `sudo chown root:video /dev/nvidia*`

                                                                                                                                                  
// vim: set syntax=asciidoc:  
