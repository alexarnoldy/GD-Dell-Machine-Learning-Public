### CURRENT STAGE OF THIS DOCUMENT: Rapidly changing (again), possibly incomplete, likely brittle do to lots of automation based on this specific configuration
#### Last updated: 2019.12.17, early AM PDT

CAUTION: The procedure outlined in this document leverages AutoYaST. AutoYaST works slightly differently depending on if the server is configured to boot in (U)EFI mode or Legacy BIOS mode. This document attempts to provide the simplest procedures for both cases. 

#### This document is a strictly guided, step-by-step implementation of a specific configuration based on the CaaS Platform v4.0.0 Deployment Guide: https://documentation.suse.com/suse-caasp/4/single-html/caasp-deployment/
* This specific configuration is based on eight nodes:
** Management Workstation - VM 
** Three CaaS Platform Master Nodes - Dell R640 hosts
** Four CaaS Platform Worker Nodes - Dell R740 hosts

.Overview of this installation procedure:
* Create a CaaS Platform Management Workstation VM and manually install the SLES15 SP1
* Manually install SLES15 SP1 on one of the Dell R640 and one of the Dell R740 hosts and configure bonded networks, as appropriate
* Make an AutoYaST clone file of each of the Dell hosts
* AutoYaST install the remaining nodes from the Dell R640 and Dell R740 AutoYaST files, as appropriate
* Prepare the nodes for forming the CaaS Platform cluster
* Form the CaaS Platform cluster
* Update the CaaS Platform cluster as needed for the application, optionally including performing GPU integration

NOTE: Unlike CaaS Platform v3, the Management Workstation for CaaS Platform v4 is not tied in a 1:1 relationship with the CaaS Platform cluster.
      A Management Workstation can be used to deploy one or more CaaS Platform clusters and a CaaS Platform cluster can be managed from the 
      Management Workstation that deployed it and/or another appropriately configured Management Workstation.

CAUTION: This guide can be used as a reference while deploying different configurations, but deviating from the specified design can cause unpredicable results


.Required preparation before beginning
* Establish an IP schema for the cluster. 
* Populate DNS records that are available to the cluster nodes
* A local RMT registration server

### Install the Management Workstation:

* Virtual Machine connected to a bridge that has access to the Internet and the cluster network
* 4 vCPU, 4096MB mem, 20GB qcow2 boot drive in default libvirt location
* Booting from ISO: Select "Installation" BUT DO NOT PRESS ENTER
* On "Boot Options:" line: `ifcfg=em1=172.29.145.53/24,172.29.145.2,172.29.202.10,suse.hpc.local hostname=kubeadmin.suse.hpc.local`
** Format of ifcfg=em1= is <node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain>
* Press Enter
* Register with the RMT server and select the SUSE CaaS Platform 4.0 module (May need to uncheck "Hide Development Modules")
* For partitioning, select Guided Setup
** A separate Home Partition is not needed on any of the CaaS Platform Nodes but can be configured, if desired
** Keep defaults except deselect "Propose Separate Swap Partition"

NOTE: Swap will likely not be needed for the Management Workstation. Swap can be used on the Management Workstations but it CANNOT be used on the CaaS Platform nodes. Swap is incompatible with Kubernetes.

* Create New User:
** User's Full Name and Username: geeko

IMPORTANT: It is recommended to use a secure password that will work with all elements of the PoC environment. Therefore a password should be selected that includes at least one upper case and one lower case letter, one number and at least one of the following three special characters: ! # $

** Select "Use this password for system administrator" 
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install


.Finish preparing the Management workstation:
* Enable passwordless sudo for the user geeko
----
sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko"
----
* `ssh-keygen`
** Accept the defaults, though a passphrase can be configured here, if desired
* Update the NTP client settings. 
** `sudo yast timezone`
*** `other Settings`
*** `Synchronize with NTP server`
*** Enter `172.29.202.15` under `NTP Server Access`
*** `Synchronize now`
**** Synchronizing with the NTP service will take several seconds but should complete without error
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`

.Create a file to reference all of the CaaS Platform cluster nodes
* `mkdir ~/autoyast_templates`
----
cat <<EOF> ~/autoyast_templates/.all_nodes
mstr1.suse.hpc.local
mstr2.suse.hpc.local
mstr3.suse.hpc.local
wrkr1.suse.hpc.local
wrkr2.suse.hpc.local
wrkr3.suse.hpc.local
wrkr4.suse.hpc.local
EOF
----

### Setup podman and the nginx webserver

** `sudo zypper -n in podman`
** Launch nginx webserver container: `sudo podman run --name autoyast-nginx -v /home/geeko/autoyast_templates:/usr/share/nginx/html:ro -P -d nginx:latest`

IMPORTANT: This container WILL NOT automatically start after rebooting the Management Workstation. Use `sudo podman start autoyast-nginx` to start it manually

* Find the network port used by the nginx container:
** `sudo podman ps`
*** The port will listed under PORTS. For example, port 32768 would be indicated with: `0.0.0.0:32768->80/tcp`
* Set this variable to the nginx port: `NGINX_PORT=""`
* Test that files in the autoyast_templates directory are available: `curl http://kubeadmin.suse.hpc.local:$NGINX_PORT/.all_nodes`
** The output should display the contents of the .all_nodes file



////
.Create an AutoYaST clone file of the Management Workstation
* `sudo yast2 clone_system`
** Approve the installation of the autoyast2 package
* `mkdir ~/autoyast_templates`
* `sudo mv /root/autoinst.xml ~/autoyast_templates/`
* `sudo chown -R geeko:users ~/autoyast_templates/`
* `cp ~/autoyast_templates/autoinst.xml ~/autoyast_templates/ses-osd-c.xml`
*** To verify the output, compare the md5sum from each of the following two commands:
**** `md5sum autoyast_templates/master.xml`
**** `curl http://ses-admin.stable.suse.lab:$NGINX_PORT/master.xml | md5sum`


.Update the master.xml AutoYaST file with the correct hostname and IP address
* `sudo zypper -n in xmlstarlet`
* `cd ~/autoyast_templates/`
* Verify that getent returns the correct IP address and fully qualified hostname 
** `getent hosts master`

WARNING: If the getent command does not return the correct IP address and fully qualified hostanme, DO NOT run the following `xml ed` and `sed` commands

* Update hostname in the master.xml file: `xml ed -L -u "//_:networking/_:dns/_:hostname" -v master master.xml`

TIP: Use the command `grep ipaddr autoinst.xml` to verify the Management Workstation's IP address

** Set this variable to the Management Workstation's IP address (i.e. 172.16.241.105): `MANAGEMENTIP=""`
----
MASTERIP=`getent hosts master | awk '{print$1}'`; sed -i "s/$MANAGEMENTIP/$MASTERIP/" master.xml
----

.Update the master.xml AutoYaST file based on architectural differences:
* If the Master Node is a bare-metal host, run this command: 
----
xml ed -L -d "//_:services-manager/_:services/_:enable/_:service[text()='spice-vdagentd']"  master.xml
----
* If the Master Node boots in Legacy BIOS mode, run this command: `xml ed -L -u "//_:bootloader/_:loader_type" -v grub2 master.xml`
* If the Master Node boots in (U)EFI mode, run this command: `xml ed -L -u "//_:bootloader/_:loader_type" -v grub2-efi master.xml`
////

////
Manual way of updating hostname and IP address
*** `cd autoyast_templates/; vim master.xml`
**** Search for <\/hostname
***** Change hostname from admin to master
**** Search for `<ipaddr`
***** Change the IP address to that of the master. In this document it is 172.16.241.105
////

////
.Update the correct boot drive for the Master Node
////

CAUTION: The following steps assume that the boot drives for all R640 nodes are configured identically. If case, edit the AutoYaST file manually to set the correct boot drive

////
* Fill the root partition with all of the remaining space on the boot drive:
** To find the <partition> element for the root partition, search for the string `<mount>\/<`
*** Inside that <partition> element (normally below the <mount> subnode), change the value of the <size> subnode to `max`
**** For example, before the change it might look like: `<size>66561507328</size>` and after the change it will look like: `<size>max</size>`
////

////
* Add the following element directly above the <services-manager> element:

----
  <scripts>
    <post-scripts config:type="list">
      <script>
        <debug config:type="boolean">true</debug>
        <feedback config:type="boolean">false</feedback>
        <feedback_type/>
        <filename>autoyast_post_updates.sh</filename>
        <interpreter>shell</interpreter>
        <location><![CDATA[http://kubeadmin.suse.hpc.local:32768/autoyast_post_updates.sh]]></location>
        <notification>Performing_Final_Updates</notification>
        <param-list config:type="list"/>
        <source><![CDATA[]]></source>
      </script>
    </post-scripts>
  </scripts>
----
** In the URL above, change the port number `32768` to the port number of your nginx webserver container
* Save the file and exit vim

*** Create the /home/geeko/autoyast_post_updates.sh file
**** ` echo "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko" > /home/geeko/autoyast_templates/autoyast_post_updates.sh `
////

### Install the first Dell R640:
* Must have access to the Internet, as well as the cluster network and the DNS, NTP, and RMT servers
* Start the node from DVD or ISO,  Select "Installation" at the DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key. Then, provide the AutoYaST parameters shown below, at the end of the "linuxefi" line (Be sure to insert a space after "splash=silent"). When ready, press Ctrl+x to boot the system.
*** AutoYaST parameters: `ifcfg=em1=172.29.145.61/24,172.29.145.2,172.29.202.10,suse.hpc.local hostname=mstr1.suse.hpc.local`
** Format of ifcfg=em1= is <node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain>
* Press Enter
* Register with the RMT server and select the SUSE CaaS Platform 4.0 module (May need to uncheck "Hide Development Modules")
* For partitioning, select Guided Setup
** A separate Home Partition is not needed on any of the CaaS Platform Nodes but can be configured, if desired
** Keep defaults except deselect "Propose Separate Swap Partition"

CAUTION: Swap is incompatible with Kubernetes.

* Create New User:
** User's Full Name and Username: geeko

IMPORTANT: It is recommended to use a secure password that will work with all elements of the PoC environment. Therefore a password should be selected that includes at least one upper case and one lower case letter, one number and at least one of the following three special characters: ! # $

** Select "Use this password for system administrator" 
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install


.Finish preparing the first Dell R640:
* Enable passwordless sudo for the user geeko
----
sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko"
----
* Update the NTP client settings. 
** `sudo yast timezone`
*** `other Settings`
*** `Synchronize with NTP server`
*** Enter `172.29.202.15` under `NTP Server Access`
*** `Synchronize now`
**** Synchronizing with the NTP service will take several seconds but should complete without error
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`

.If needed, adjust the first Dell R640's networking after it completes installation

NOTE: This document demonstrates the procedure for creating a bonded network from em1
    and em2, then assigning the node's IP address to that bond; however, your configuration may be different

NOTE: VM based CaaS Platform nodes will likely not need any network modifications

* Perform the following steps from the Master Node's console:

TIP: In yast, Tab will help you navigate through panes and options. Each option in yast will have a letter highlighted.
     Using "Alt" + that letter will directly open that option.

** `sudo yast lan`
** `(Use tab and the arrow keys to highlight em1) -> Delete -> OK`
** `sudo yast lan`
** `Add -> Device Type -> Bond -> Next`
** `(Select Statically Assigned IP Address) -> IP Address -> (input the Master Node's IP address)`
** `(Adjust the Subnet Mask, if needed) -> Bonded Slaves -> Yes`
** `(Select both em1 and em2) -> Next`
** `Routing -> (Ensure the Device for Default IPv4 Gateway is "-") -> OK`
* Verify networking is functioning correctly:
** `ip a`
** `ping opensuse.com`

.Ensure swap is not enabled. Swap is incompatible with Kubernetes
* `cat /proc/swaps`
** Should return a header line, but nothing else
* `grep swap /etc/fstab`
** Should return nothing
*** If swap is enabled, remote the swap line from the /etc/fstab file and reboot


### Install the first Dell R740:
* Must have access to the Internet, as well as the cluster network and the DNS, NTP, and RMT servers
* Start the node from DVD or ISO,  Select "Installation" at the DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key. Then, provide the AutoYaST parameters shown below, at the end of the "linuxefi" line (Be sure to insert a space after "splash=silent"). When ready, press Ctrl+x to boot the system.
*** AutoYaST parameters: `ifcfg=em1=172.29.145.71/24,172.29.145.2,172.29.202.10,suse.hpc.local hostname=wrkr1.suse.hpc.local`
** Format of ifcfg=em1= is <node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain>
* Press Enter
* Register with the RMT server and select the SUSE CaaS Platform 4.0 module (May need to uncheck "Hide Development Modules")
* For partitioning, select Guided Setup
** A separate Home Partition is not needed on any of the CaaS Platform Nodes but can be configured, if desired
** Keep defaults except deselect "Propose Separate Swap Partition"

CAUTION: Swap is incompatible with Kubernetes.

* Create New User:
** User's Full Name and Username: geeko

IMPORTANT: It is recommended to use a secure password that will work with all elements of the PoC environment. Therefore a password should be selected that includes at least one upper case and one lower case letter, one number and at least one of the following three special characters: ! # $

** Select "Use this password for system administrator" 
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install


.Finish preparing the first Dell R740:
* Enable passwordless sudo for the user geeko
----
sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko"
----
* Update the NTP client settings. 
** `sudo yast timezone`
*** `other Settings`
*** `Synchronize with NTP server`
*** Enter `172.29.202.15` under `NTP Server Access`
*** `Synchronize now`
**** Synchronizing with the NTP service will take several seconds but should complete without error
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`

.If needed, adjust the first Dell R740's networking after it completes installation

NOTE: This document demonstrates the procedure for creating a bonded network from em1
    and em2, then assigning the node's IP address to that bond; however, your configuration may be different

////
NOTE: VM based CaaS Platform nodes will likely not need any network modifications
////

* Perform the following steps from the Dell R740's console:

TIP: In yast, Tab will help you navigate through panes and options. Each option in yast will have a letter highlighted.
     Using "Alt" + that letter will directly open that option.

** `sudo yast lan`
** `(Use tab and the arrow keys to highlight em1) -> Delete -> OK`
** `sudo yast lan`
** `Add -> Device Type -> Bond -> Next`
** `(Select Statically Assigned IP Address) -> IP Address -> (input the Master Node's IP address)`
** `(Adjust the Subnet Mask, if needed) -> Bonded Slaves -> Yes`
** `(Select both em1 and em2) -> Next`
** `Routing -> (Ensure the Device for Default IPv4 Gateway is "-") -> OK`
* Verify networking is functioning correctly:
** `ip a`
** `ping opensuse.org`

.Ensure swap is not enabled. Swap is incompatible with Kubernetes
* `cat /proc/swaps`
** Should return a header line, but nothing else
* `grep swap /etc/fstab`
** Should return nothing
*** If swap is enabled, remote the swap line from the /etc/fstab file and reboot

### Create an AutoYaST clone file of the Dell R640 and Dell R740

NOTE: Peform the following steps on each of the two installed servers
* `sudo yast2 clone_system`
** Approve the installation of the autoyast2 package
* `sudo mv /root/autoinst.xml ~/$(hostname -f).xml`
* `sudo chown -R geeko:users ~/$(hostname -f).xml`
* `scp ~/$(hostname -f).xml kubeadmin.suse.hpc.local:~/autoyast_templates/`

## Need to copy the files and adjust the hostnames/IP addresses

### AutoYaST install the remaining Dell R640 and R740 servers

NOTE: Perform the following steps on each of the remaining Dell servers, adjusting the IP address and hostname portions of the AutoYaST parameters below

IMPORTANT: The procedure for installing via AutoYaST is slightly different depending on if the target server is configured to boot in (U)EFI mode or Legacy BIOS mode. Be sure to verify the boot mode for a bare-metal server before continuing. Virtual Machines commonly boot in Legacy BIOS mode. For more information, see the SLES15 SP1 AutoYaST guide: https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast

* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the BIOS of the Master Node
* Start the Master Node from DVD or ISO,  Select "Installation" at the DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key. Then, provide the AutoYaST parameters shown below, at the end of the "linuxefi" line (Be sure to insert a space after "splash=silent"). When ready, press Ctrl+x to boot the system.
*** AutoYaST parameters: `autoyast=http://kubeadmin.suse.hpc.local:<nginx port>/<node name>.xml ifcfg=em1=<node IP address>/24,<IP of gateway>,<IP of DNS server>,suse.hpc.local hostname=<node name>.suse.hpc.local`


.Add Master Node SSH key to its own authorized_keys file so it will be included in the AutoYaST clone file
* `ssh-keygen`
** Accept the defaults
* `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`

.Creating an AutoYaST clone of the Master Node
** The following steps can be performed from the Master Node's console or an SSH session
*** `sudo yast2 clone_system`
*** SCP the AutoYaST file to the Management Workstation. This will overwrite the original master.xml file. Make a copy first, if needed.
**** ` sudo scp /root/autoinst.xml kubeadmin.suse.hpc.local:/home/geeko/autoyast_templates/master.xml `

.Create copies of the master.xml file for each Worker Node

TIP: Perform the following steps from the Management Workstation as the geeko user

* `cd ~/autoyast_templates/`
* `for EACH in 1 2 3; do cp -p master.xml worker$EACH.xml; done`

.Edit each Worker Node XML file to update the hostname and IP address
////
Note: Due to the "<profile xmlns=" default namespace declaration in the AutoYaST file, xmlstarlet selects and edits follow a different format:
To select the hostname: xml sel -t -m "//_:networking/_:dns" -v _:hostname FILENAME.xml
To update the hostname: xml ed -L -u "//_:networking/_:dns/_:hostname" -v <new hostname> FILENAME.xml
////

* Change the hostname value for each Worker Node
** `for EACH in 1 2 3; do xml ed -L -u "//_:networking/_:dns/_:hostname" -v worker$EACH worker$EACH.xml; done`
* Verify that getent returns the correct IP addresses and hostnames. If not, DO NOT run the subsequent xml ed for loop
** `for EACH in 1 2 3; do getent hosts worker$EACH; done`
* Change the ipaddr value for each Worker Node's external interface
** Set this variable to the Master Node's IP address: `MASTERIP=""`
** `for EACH in 1 2 3; do WORKERIP=`getent hosts worker$EACH | awk '{print$1}'`; sed -i "s/$MASTERIP/$WORKERIP/" worker$EACH.xml; done`

////
This was the manual way to update hostname and IP address
** `for EACH in 1 2 3; do vim worker$EACH.xml; done`
*** Search for <\/hostname
**** Change hostname from master to the correct Worker Node name
*** Search for <ipaddr
**** Change the IP address to that of the correct Worker Node
*** Use the command `:x` to save the file and move on the the next
////

.Test that each Worker Node XML file is available through the nginx webserver
* `docker ps`
* Set this variable to the port listed under PORTS: `NGINX_PORT=""`
* Test that each Worker Node autoyast file is available: `for EACH in 1 2 3; do curl http://kubeadmin.suse.hpc.local:$NGINX_PORT/worker$EACH.xml | egrep "<hostname|ipaddr"; done`
** Verify each hostname and IP address is correct for each Worker Node

.AutoYaST install worker1

IMPORTANT: The procedure for installing via AutoYaST is slightly different depending on if the target server is configured to boot in (U)EFI mode or Legacy BIOS mode. To ensure a the boot mode for a bare-metal server, consult its BIOS before continuing. Virtual Machines commonly boot in Legacy BIOS mode. For more information, see the SLES15 SP1 AutoYaST guide: https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast

TIP: It is recommended to fully install worker1 before continuing to the rest of the Worker Nodes.
     Once it is shown that worker1 can be fully installed with the AutoYaST configuration, multiple Worker Nodes can be installed simultaneously.

* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the VM or host BIOS
* Start the worker1 from DVD ISO,  Select "Installation" at DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key, then provide the AutoYaST parameters, shown below; at the end of the "linuxefi" line. When ready, press Ctrl+x to boot the system.
*** AutoYaST parameters: `autoyast=http://kubeadmin.suse.hpc.local:<nginx port>/<worker node name>.xml ifcfg=em1=<IP of worker node>/24,<IP of gateway>,<IP of DNS server>,suse.hpc.local hostname=<worker node name>.suse.hpc.local

.AutoYaST install the rest of the Worker Nodes
* Repeat the previous step, "AutoYast install worker1" for each of the remaining Worker Nodes

### Preparation for forming CaaS Platform cluster

NOTE: The following commands should be run from the Management Workstation and require a .all_nodes file that contains the fully qualified hostnames of all cluster nodes. Create this file if it doesn't already, before continuing.
 
* `eval "$(ssh-agent)"`
* `ssh-add /home/geeko/.ssh/id_rsa`
* Verify passwordless SSH and sudo capabilities for the geeko user on all nodes
----
for EACH in `cat .all_nodes`; do ssh $EACH sudo hostname -f; done
----
** It should return each fully qualified hostname with no additional interaction required

.Ensure caasp, SLES, basesystem, and containers are all "Registered"
----
for EACH in `cat .all_nodes`; do echo $EACH; ssh $EACH sudo SUSEConnect -s | egrep --color "caasp|SLES|basesystem|containers|\"Registered\"" && echo"" && echo "Press Enter for next system" && read NEXT; done
----

.Ensure swap is not enabled on any of the CaaS Platform hosts
----
for EACH in `cat .all_nodes`; do echo $EACH; ssh $EACH cat /proc/swaps; echo ""; done
----
** Should return a header line for each node, but nothing else

### Bootstrap the CaaS Platform cluster

////
.For any VM nodes, snapshot before instantiating the cluster
* Create snapshot
----
for EACH in `cat .all_nodes`; do echo $EACH; ssh $EACH sudo virsh snapshot-create-as $EACH --name "before forming cluster"; echo ""; done
----
////

* On the Management Workstation:
* `sudo zypper -n install -t pattern SUSE-CaaSP-Management`
* `skuba cluster init --control-plane mstr.suse.hpc.local suse-caasp-hpc`
** Note: --control-plane defines the FQDN of the load balancer. The second argument is the name of the cluster.
* Ensure the SSH Agent is running and has the geeko user's RSA key loaded
** `ps -ef | grep ssh-agent`
*** If this doesn't return an ssh-agent running for the geeko user, run the following commands to start ssh-agent and add the Management Workstation's key:
**** `eval "$(ssh-agent)"`
**** `ssh-add /home/geeko/.ssh/id_rsa`
* `cd /home/geeko/suse-caasp-hpc`
* `skuba node bootstrap --user geeko --sudo --target mstr1.suse.hpc.local mstr1`
** Note this command bootstraps the CaaS Platform cluster with mstr1.suse.hpc.local as the first master node. Internally, Kubernetes will assign this node the name "mstr1"

### Join additional Master Nodes to the cluster
* To join a single Master Node to the cluster:
----
eval "$(ssh-agent)"
ssh-add /home/geeko/.ssh/id_rsa
export MASTER_FQDN=
MASTER=`echo $MASTER_FQDN | awk -F. '{print$1}'`; \
skuba node join --role master --user geeko --sudo \
--target $MASTER_FQDN $MASTER
----

* To join all remainging Master Nodes in the .all_nodes file:
----
eval "$(ssh-agent)"
ssh-add /home/geeko/.ssh/id_rsa
for MASTER_FQDN in `grep mstr .all_nodes | tail -n+2`; do \
MASTER=`echo $MASTER_FQDN | awk -F. '{print$1}'`; \
skuba node join --role master --user geeko --sudo \
--target $MASTER_FQDN $MASTER; \
done
----

IMPORTANT: If any nodes will require additional configuration such as GPU integration, use the command `kubectl cordon <node name>` to prevent work from being assigned to it until it is ready.

### Join Worker Nodes to the cluster
* To join a single Worker Node to the cluster:
----
eval "$(ssh-agent)"
ssh-add /home/geeko/.ssh/id_rsa
export WORKER_FQDN=
WORKER=`echo $WORKER_FQDN | awk -F. '{print$1}'`; \
skuba node join --role worker --user geeko --sudo \
--target $WORKER_FQDN $WORKER
----

* To join all remainging Worker Nodes in the .all_nodes file:
----
eval "$(ssh-agent)"
ssh-add /home/geeko/.ssh/id_rsa
for WORKER_FQDN in `grep wrkr .all_nodes`; do \
WORKER=`echo $WORKER_FQDN | awk -F. '{print$1}'`; \
skuba node join --role worker --user geeko --sudo \
--target $WORKER_FQDN $WORKER; \
done
----

IMPORTANT: If any nodes will require additional configuration such as GPU integration, use the command `kubectl cordon <node name>` to prevent work from being assigned to it until it is ready.


.Verify the status of the cluster
* `cd /home/geeko/suse-caasp-hpc`
* `skuba cluster status`

.Enable the use of kubectl from the Management Workstation
* `echo export KUBECONFIG=/home/geeko/caaspv4-cluster/admin.conf >> ~/.bashrc`
* `. ~/.bashrc` 
* `kubectl get nodes`

////
.For any VM nodes, snapshot immediately after instantiating the cluster
* Create snapshot script
----
cat <<EOF> /tmp/snap_after_cluster.sh
/bin/bash
sudo virsh list --all | grep "suse.hpc.local" | awk '{print$2}' > /tmp/k8s_nodes
for K8S_NODE in `cat /tmp/k8s_nodes`; do sudo virsh snapshot-create-as \$K8S_NODE --name "after forming cluster"; done
EOF
----

* scp the script to each node and execute it
----
for EACH in `cat .all_kvm_hosts`; do \
echo $EACH; scp /tmp/snap_after_cluster.sh $EACH:/tmp; \
ssh $EACH /tmp/snap_after_cluster.sh ; echo ""; \
done
----
////



### Configure Dex and Gangway

.Create the ClusterRoleBinding for the admins group
* `kubectl create clusterrolebinding ldap-admin-access --clusterrole=cluster-admin --group=admins`
* Edit the configmap: `kubectl --namespace=kube-system edit configmap oidc-dex-config`
* Restart Dex and Gangway pods:
----
kubectl --namespace=kube-system delete pod -l app=oidc-dex
kubectl --namespace=kube-system delete pod -l app=oidc-gangway
----
* Verify the new pods have started correctly: 
----
kubectl get pods -n kube-system -l app=oidc-dex
kubectl get pods -n kube-system -l app=oidc-gangway
----

.Add the Dex self-signed cert to the Admin node:
* Get the cert from the dex pod: `kubectl exec -it -n kube-system $(kubectl get pod -n kube-system -l app=oidc-dex -o name | head -1) cat /etc/dex/pki/ca.crt > /tmp/mstr.suse.hpc.local:32000-ca.crt`
* Move the cert into place: `sudo mv /tmp/mstr.suse.hpc.local:32000-ca.crt /etc/pki/trust/anchors/`
* Update the certs: `sudo update-ca-certificates`

.Test authentication:
* `skuba auth login -s https://mstr.suse.hpc.local:32000`
* Remove the kubeconf.txt file, if the login was successful: `rm kubeconf.txt`
* Use the Chromium browser to test logging into Gangway at https://mstr.suse.hpc.local:32001
** Log in with a user id, such as suse, rather than email address
** Try using Incognito Mode if cookie errors prevent logging in

### Configure the Ceph storage class



### Enable privilege escalation on CaaS Platform

.Edit the pod security policy
* `kubectl edit psp suse.caasp.psp.unprivileged`
* Add the following elements below `spec:`
----
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - SYS_PTRACE
  - NET_ADMIN
----



### Enable GPU availability for CaaS Platform

.Start on the Nvidia GPU equiped Worker Node

* Verify the model of Nvidia GPU:
* `sudo lspci | grep -i nvidia`
** Check against: https://developer.nvidia.com/cuda-gpus to ensure the GPU is CUDA compatible

* Install the appropriate kernel header files for the kernel version
** `uname -r`
*** Output is in the form of <version>-<variant>, i.e. 4.12.14-197.26-default
** `sudo zypper --non-interactive install kernel-default-devel=4.12.14-197.26`
*** Format is sudo zypper --non-interactive install kernel-<variant>-devel=<version>

* Install the Cuda toolkit:
----
sudo zypper addrepo http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/cuda-sles15.repo
sudo SUSEConnect --product PackageHub/15.1/x86_64
sudo SUSEConnect --product sle-module-desktop-applications/15.1/x86_64
sudo zypper refresh     # May require "trust always" the "package signing key"
sudo zypper --non-interactive install cuda
----

* Verify which packages and versions of the Cuda toolkit were installed `sudo zypper search cuda`

* When the driver is correctly loaded it will show the version in: `cat /proc/driver/nvidia/version`
** If the driver hasn't loaded, reboot the node and check again

* Check that you can access the GPU:
----
sudo usermod -G video -a geeko
sudo usermod -G video -a root
sudo su - geeko
nvidia-smi
----
** Should get an output that contains:
----
NVIDIA-SMI XXX.YY Driver Version: XXX.YY CUDA Version: XX.Y
. . . .
No running processes found
----

* Install the Nvidia libnvidia-container:
----
wget https://github.com/NVIDIA/libnvidia-container/releases/download/v1.0.0/libnvidia-container_1.0.0_x86_64.tar.xz
tar xJf libnvidia-container_1.0.0_x86_64.tar.xz
sudo cp libnvidia-container_1.0.0/usr/local/bin/nvidia-container-cli /usr/bin
sudo cp libnvidia-container_1.0.0/usr/local/lib/libnvidia-container.so* /usr/lib64
----
* Verify functionality of the nvidia-container-cli utility: `nvidia-container-cli info`
** Should get an output that contains:
----
NVRM version:   XXX.YY                                                          
CUDA version:   XX.Y  
Model:          X
Brand:          Y
----

### Install nvidia-container-toolkit on any nodes equiped with GPUs

.Start on the CaaS Platform Administrative Workstation
* Download the required package via a CentOS container:
----
sudo zypper --non-interactive install podman
sudo podman run --rm -ti -v$PWD:/var/tmp centos:7
DIST=$(. /etc/os-release; echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$DIST/nvidia-container-runtime.repo |    tee /etc/yum.repos.d/nvidia-container-runtime.repo
yum install --downloadonly nvidia-container-runtime-hook        # May have answer "y" to accept Nvidia's GPG key
cp /var/cache/yum/x86_64/7/nvidia-container-runtime/packages/nvidia-container-toolkit-1.0.5-2.x86_64.rpm /var/tmp
exit
----

* Create the unrpm script from: https://github.com/openSUSE/obs-build/blob/master/unrpm
* Unpack the rpm: `bash unrpm nvidia-container-toolkit-1.0.5-2.x86_64.rpm`
* If running this on another node (i.e. the Administrative Workstation), SCP the files to the GPU Worker Node:
** Set this variable to the FQDN of the GPU Worker Nodes `export WORKER=`
** SCP the files to the Worker Node:
----
scp -r etc/ $WORKER:~/
scp -r usr/ $WORKER:~/
----

.Continue the process on the GPU Worker Node:

* Copy the Nvidia Container Toolkit into place:
----
sudo mkdir -p /etc/nvidia-container-runtime/
sudo mkdir -p /usr/libexec/oci/hooks.d/
sudo mkdir -p /usr/share/licenses/nvidia-container-toolkit-1.0.5/

sudo cp etc/nvidia-container-runtime/config.toml /etc/nvidia-container-runtime/config.toml
sudo cp usr/bin/nvidia-container-toolkit /usr/bin/nvidia-container-toolkit
sudo cp usr/share/containers/oci/hooks.d/oci-nvidia-hook.json /usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
sudo cp usr/libexec/oci/hooks.d/oci-nvidia-hook /usr/libexec/oci/hooks.d/oci-nvidia-hook
sudo cp usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE /usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE
----

* Edit  the /etc/nvidia-container-runtime/config.toml file to uncomment or insert the line: `user = "root:video"`

* Update the metadata of the Nvidia device files:
----
sudo chmod 0666 /dev/nvidia*
sudo chown root:video /dev/nvidia*
----

* (Optional) Test that a container can access the GPU: `sudo podman --rm run nvidia/cuda nvidia-smi`

.Finish the process from the Administrative Workstation
* Install the Nvidia Kubernetes device plugin 
* `kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml`

* Set this variable for the next several commands: `export WORKER=`

* Ensure the correct number of GPUs are recognized on the worker node: `kubectl describe node $WORKER | egrep "gpu|Unschedulable"`
** Output should include three lines beginning with `nvidia.com/gpu`. The first two should match the number of GPUs on the node. The last line should show quanties zero

NOTE: If the previous command also showed `Unschedulable` as `true`, uncordon the node before continuing: `kubectl uncordon $WORKER`

* Ensure that CaaS Platform can run a GPU enabled pod on the node:

* Set this variable to the number of GPUs on this node: `export GPUS=`
* Create the cuda-vector-add.yaml file:
----
cat <<EOF> cuda-vector-add.yaml
apiVersion: v1                                                                  
kind: Pod                                                                       
metadata:                                                                       
  name: cuda-vector-add                                                         
spec:                                                                           
  restartPolicy: OnFailure                                                      
  nodeSelector:
    kubernetes.io/hostname: $WORKER
  containers:                                                                   
    - name: cuda-vector-add                                                     
      # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile
      image: "k8s.gcr.io/cuda-vector-add:v0.1"                                  
      resources:                                                                
        limits:                                                                 
          nvidia.com/gpu: $GPUS
EOF
----

* Apply the pod creation file and review the pod's logs and node assignment: `kubectl apply -f cuda-vector-add.yaml && kubectl logs cuda-vector-add && kubectl get pods -o wide | grep cuda-vector-add`
** Output should include phrases such as `CUDA kernel launch` and `Test PASSED`, as well as show that the pod ran on this node
* Remove the pod: `kubectl delete -f cuda-vector-add.yaml`




### Troubleshooting Section

.Troubleshoot a failed bootstrap
* ssh to master and `sudo less /var/log/messages` 
* Search for kub
* Follow the progression of the skuba command and kubeadm
** Generally skuba will install the packages, then launch kubeadm
** kubeadm will set up the K8s components
** If the failure occurs after kubeadm takes over try to replicate the failure:
*** scp kubeadm-init.conf from the cluster directory (caaspv4-cluster in this doc) to /tmp on the master node
*** Run the `kubeadm init` command that is in /var/log/messages
*** kubeadm should give reasonably actionable error messages






// vim: set syntax=asciidoc:
