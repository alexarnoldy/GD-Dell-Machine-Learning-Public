### CURRENT STAGE OF THIS DOCUMENT: Beta, likely brittle do to lots of automation based on a specific configuration

CAUTION: The procedure outlined in this document leverages AutoYaST. AutoYaST works slightly differently depending on if the server to be installed is configured to boot in (U)EFI mode or Legacy BIOS mode. This document attempts to provide the simplest procedures for both cases. 

#### This document is a strictly guided, step-by-step implementation of a specific configuration based on the CaaS Platform v4.0.0 Deployment Guide: https://documentation.suse.com/suse-caasp/4/single-html/caasp-deployment/
* The specific configuration of this design is based on a minimum of eight nodes:
** A Management Workstation - VM 
** Three CaaS Platform Master Nodes - Dell R640 hosts
** Four CaaS Platform Worker Nodes - Dell R740 hosts

==== Overview of this installation procedure:
. <<anchor-10>>
. <<anchor-20>>
. <<anchor-30>>
. <<anchor-40>>
. <<anchor-50>>
. <<anchor-60>>
. <<anchor-65>>
. <<anchor-70>>
. <<anchor-80>>
. <<anchor-90>>
. <<anchor-100>>
. <<anchor-110>>
. <<anchor-120>>

NOTE: Unlike CaaS Platform v3, the Management Workstation for CaaS Platform v4 is not tied in a 1:1 relationship with the CaaS Platform cluster.
      A Management Workstation can be used to deploy one or more CaaS Platform clusters and a CaaS Platform cluster can be managed from the 
      Management Workstation that deployed it and/or another appropriately configured Management Workstation.

CAUTION: This guide can be used as a reference while deploying different configurations, but deviating from the specified design can cause unpredicable results


==== Required preparation before beginning
* Establish an IP schema for the cluster
** In addition to the Management Workstation and cluster nodes, a virtual (floating) IP address will be needed for the Kubernetes API service
** Additional IP addresses needed may include those for one or more load balancers and a software management (RMT) server
* Populate a DNS service that is available to the cluster nodes
* Optionally, a DHCP service that is available to the cluster nodes
** The instructions below include deploying a local dnsmasq container to provide DNS or DHCP services to the cluster
* A local RMT registration server
** A "Repository Mirroring Tool Guide" for deploying a local RMT server can be found under the appropriate version of "SUSE Linux Enterprise Server" at: https://documentation.suse.com/
*** Additional, but unsupported, guidance can be found here: https://github.com/alexarnoldy/SUSE-tools/blob/master/Configure_RMT_Server.adoc

[[anchor-10]]
=== Install the Management Workstation VM

* A KVM/qemu Virtual Machine connected to a bridge that has access to the Internet as well as the cluster network
* Resources of 4 vCPU, 4096MB mem, 20GB qcow2 boot drive in default libvirt location
* When booting from ISO: Select "Installation" BUT DO NOT PRESS ENTER
* On the "Boot Options:" line provide the network configuration parameters in the form of `ifcfg=eth0=<node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain>`
** For example: `ifcfg=eth0=172.29.145.53/24,172.29.145.2,172.29.202.10,suse.hpc.local hostname=kubeadmin.suse.hpc.local`
* Press Enter
* Install SUSE Linux Enterprise Server
* Register with the SMT/RMT server and select the SUSE CaaS Platform 4.0 module (May need to uncheck "Hide Development Versions")
* For partitioning, select Guided Setup
** Do not enable LVM or encryption
** A separate Home Partition is not needed on any of the CaaS Platform Nodes but can be configured, if desired
** Deselect "Propose Separate Swap Partition"

NOTE: Swap will likely not be needed for the Management Workstation. Swap can be used on the Management Workstations but it CANNOT be used on the CaaS Platform nodes. Swap is incompatible with Kubernetes.

* Select the appropriate timezone, then select `Other Settings...`
** Select `Synchronize with NTP Server`
** Provide the `NTP Server Address`
** Select `Synchronize now`
** Check the boxes for `Run NTP as daemon` and `Save NTP Configuration`
** Select `Accept`

CAUTION: The remaining procedures in this document assume a username "geeko" for several important steps. If a different username is preferred, take care to adjust those steps later in this document, especially around initializing and bootstrapping the CaaS Platform cluster.

* Create New User:
** User's Full Name and Username: geeko

IMPORTANT: It is recommended to use a secure password that will work with all elements of the PoC environment. Therefore a password should be selected that is at least eight characters long, includes at least one upper case and one lower case letter, one number and at least one of the following three special characters: ! # $

** Select "Use this password for system administrator" 
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install


==== Finish preparing the Management workstation:
* Enable passwordless sudo for the user geeko
----
sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko"
----
* `ssh-keygen`
** Accept the defaults, though a passphrase can be configured here, if desired
* Update the NTP client settings. 
** `sudo yast timezone`
*** `other Settings`
*** `Synchronize with NTP server`
*** Enter `172.29.202.15` under `NTP Server Access`
*** `Synchronize now`
**** Synchronizing with the NTP service will take several seconds but should complete without error
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`

==== Create a file to reference all of the CaaS Platform cluster nodes

IMPORTANT: These procedures utilize the pattern "mstr" to identify master nodes and "wrkr" to identify worker nodes. In addition, the CaaS Platform cluster name will be set to the fully qualified domain name of the nodes. Configure the .all_nodes file accordingly. If these conventions are not desired, take care to adjust the procedures covered later in this document; specifically but not limited to bootstrapping the cluster.

* `mkdir ~/autoyast_templates`
----
cat <<EOF> ~/autoyast_templates/.all_nodes
mstr1.suse.hpc.local
mstr2.suse.hpc.local
mstr3.suse.hpc.local
wrkr1.suse.hpc.local
wrkr2.suse.hpc.local
wrkr3.suse.hpc.local
wrkr4.suse.hpc.local
EOF
----

==== Optionally, setup a local DNS/DHCP container

TIP: This step reduces the amount of typing (and thus the chances of typos) at the GRuB line when AutoYaST installing the cluster nodes, but is not required to successfully install the cluster. 

CAUTION: It is highly recommended to verify there is not an existing DHCP server on the subnet before continuing (See the Troubleshooting section in the opensuse-dnsmasq-container link below)

TIP: This step is primarily intended to support AutoYaST installation. However, if an external DNS server is not available, this DNS/DHCP container can be used to support the installed and configured CaaS Platform cluster.

* After cloning the repository in the link below, update the dnsmasq_hosts file with at least one entry that points to the Management Workstation, such as `172.29.145.53    kubeadmin`
** Eventually all cluster nodes, plus the Management Workstation and all load balancers will need to be resolve all members of the cluster environment
* Ensure the DHCP configuration in the dnsmasq.conf file includes the correct default router and points to the Management Workstation's cluster network IP address for DNS resolution

* Follow the README.adoc file to create a DNS+DHCP https://github.com/alexarnoldy/opensuse-dnsmasq-container[openSUSE Dnsmasq container]

==== Optionally, setup a local NTP container

* After cloning the repository in the link below, update the chrony.conf file

* Follow the README.adoc file to create a chrony https://github.com/alexarnoldy/opensuse-chrony-container[openSUSE Chrony container]

==== Setup the nginx webserver container to serve AutoYaST templates

* `sudo zypper -n in podman`
* Verify that port 80 on this host is not currently in use: `ss -npr --listening | grep :80`

IMPORTANT: If port 80 is in use, specify a different port with the `-p <container port>:<host port>`` option, or a random high port with the `-P` option in the following command

* Launch nginx webserver container: `cd ~; sudo podman run --name autoyast-nginx -v /home/geeko/autoyast_templates:/usr/share/nginx/html:ro -p 80:80 -d nginx:latest`

IMPORTANT: This container WILL NOT automatically start after rebooting the Management Workstation. Use `sudo podman start autoyast-nginx` to start it manually

NOTE: If the webserver on the Managment Workstation is using a port other than 80, specify that port in the curl command below, i.e. `curl http://kubeadmin:<port>/.all_nodes`. The port can be verified with `sudo podman ps`

* Test that files in the autoyast_templates directory are available on the cluster network (if possible, from another system connected to the clsuter network): `curl http://kubeadmin/.all_nodes`
** The output should display the contents of the .all_nodes file

TIP: In some cases the command above will fail to resolve to the correct IP address for the Management Workstation. In those cases, simply use the Management Workstation's cluster network IP address.


[[anchor-20]]
=== Install SLES15 SP1 on the first of each type of node to be used in the cluster (i.e. Dell R640 and Dell R740)
* Nodes must have access to the Internet, as well as the cluster network; and the DNS, NTP, and RMT servers
* Start the node from DVD or ISO,  Select "Installation" at the DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the Network configuration parameters, as shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key. Then, provide the Network configuration parameters as shown below, at the end of the "linuxefi" line (Be sure to insert a space after "splash=silent"). When ready, press Ctrl+x to boot the system.
*** Network configuration parameters: `ifcfg=em1=<node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain> hostname=<FQDN of node>`
** For example: `ifcfg=em1=172.29.145.61/24,172.29.145.2,172.29.202.10,suse.hpc.local hostname=mstr1.suse.hpc.local`
* Register with the RMT server and select the SUSE CaaS Platform 4.0 module (May need to uncheck "Hide Development Modules" to see it)
* For partitioning, select Guided Setup
** For best performance select XFS filesystem for the root partition
** A separate Home Partition is not needed on any of the CaaS Platform Nodes but can be configured, if desired
** Deselect "Propose Separate Swap Partition"

CAUTION: Swap is incompatible with Kubernetes.

* Select the appropriate timezone, then select `Other Settings...`
** Select `Synchronize with NTP Server`
** Provide the `NTP Server Address`
** Select `Synchronize now`
** Check the boxes for `Run NTP as daemon` and `Save NTP Configuration`
** Select `Accept`

* Create New User:
** User's Full Name and Username: geeko

IMPORTANT: It is recommended to use a secure password that will work with all elements of the PoC environment. Therefore a password should be selected that includes at least one upper case and one lower case letter, one number and at least one of the following three special characters: ! # $

** Select "Use this password for system administrator" 
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install


==== Finish preparing the first Dell servers:

* From the Management Workstation, add the geeko@kubeadmin SSH credentials to the server: `ssh-copy-id -i /home/geeko/.ssh/id_rsa.pub <hostname>`

NOTE: Peform the following steps on each of the two installed servers

.Enable passwordless sudo access for the geeko user
* `sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' > /etc/sudoers.d/01geeko"`

////
Shouldn't be needed now that it's done during the install
.Update the NTP client settings 
** `sudo yast timezone`
*** Ensure the selected timezone is correct
*** `other Settings`
*** `Synchronize with NTP server`
*** Enter `172.29.202.15` under `NTP Server Access`
*** `Synchronize now`
**** Synchronizing with the NTP service will take several seconds but should complete without error
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`
////

==== If needed, adjust the first Dell servers' networking after they complete installation

NOTE: This document demonstrates the procedure for creating a bonded network from em1
    and em2, then assigning the node's IP address to that bond; however, your configuration may be different

NOTE: Peform the following steps on each of the two installed servers

TIP: Perform the following steps from the server's console

TIP: In yast, Tab will help you navigate through panes and options. Each option in yast will have a letter highlighted.
     Using "Alt" + that letter will directly open that option.

** `sudo yast lan`
** `(Use tab and the arrow keys to highlight em1) -> Delete -> OK`
** `sudo yast lan`
** `Add -> Device Type -> Bond -> Next`
** `(Select Statically Assigned IP Address) -> IP Address -> (input the server's IP address)`
** `(Adjust the Subnet Mask, if needed) -> Bonded Slaves -> Yes`
** `(Select both em1 and em2) -> Next`
** `Routing -> (Ensure the Device for Default IPv4 Gateway is "-") -> OK`
* Verify networking is functioning correctly:
** `ip a`
** `ping opensuse.com`

==== Ensure swap is not enabled. Swap is incompatible with Kubernetes
* `cat /proc/swaps`
** Should return a header line, but nothing else
* `grep swap /etc/fstab`
** Should return nothing
*** If swap is enabled, remote the swap line from the /etc/fstab file and reboot

[[anchor-30]]
=== Prepare the AutoYaST files for all of the cluster nodes

==== Create an AutoYaST clone file of the first Dell R640 and Dell R740 servers

NOTE: Peform the following steps on each of the two installed servers

* `sudo yast2 clone_system`
** Approve the installation of the autoyast2 package
* `sudo mv /root/autoinst.xml ~/$(hostname).xml`
* `sudo chown -R geeko:users ~/$(hostname).xml`
* `scp ~/$(hostname).xml kubeadmin.suse.hpc.local:~/autoyast_templates/`


==== Create the AutoYaST files for the remaining nodes  

NOTE: Peform the following steps on the Management Workstation

* Make a copy of the first servers' AutoYaST file for each of the remaining nodes, naming them with the hostname of the node and ending in .xml
** An example of this operation, might be:
----
cd ~/autoyast_templates/
for NODE in mstr2 mstr3; do cp -p mstr1.xml ${NODE}.xml; done
for NODE in wrkr2 wrkr3 wrkr4; do cp -p wrkr1.xml ${NODE}.xml; done
----

CAUTION: Take care not to mix up the two different types of servers. Applying the Dell R640 AutoYaST file to a Dell R740, or vise versa, could yeild unexpected results.

==== Adjust the XML files to with the correct hostnames and IP addresses of the cluster nodes

IMPORTANT: Before continuing, ensure each cluster node has an XML file as well as an entry in the .all_nodes file

NOTE: Peform the following steps once for each of the two installed servers (i.e. once for the Dell R640, then again for the Dell R740)

* Update the hostnames in the nodes' XML files:
** Set this variable to the hostname (not the FQDN) of the installed server (i.e. the installed Dell R640): `export FIRST_HOSTNAME=`
** Execute this loop:
----
cd ~/autoyast_templates
rm -f /tmp/hostname_update 
for EACH in $(ls -1 mstr* wrkr*)
do 
NEW_HOSTNAME=$(echo ${EACH} | awk -F. '{print$1}')
echo "sed -i 's/${FIRST_HOSTNAME}/${NEW_HOSTNAME}/g' ${EACH}" >> /tmp/hostname_update
done
----
** Review the /tmp/hostname_update file to ensure the sed commands to change the hostnames are correct: `grep -v suse /tmp/hostname_update`
*** Correct any mistakes in the file, then execute the commands in it: `bash /tmp/hostname_update`
* Update the IP addresses in the XML files:
** Set this variable to the IP address of the installed server: `export FIRST_IP=`

TIP: Get the correct IP address with the command: `grep ipaddr <hostname>.xml` where <hostname> is the first node installed
** Execute this loop:
----
cd ~/autoyast_templates
rm -f /tmp/IP_update 
for EACH in $(ls -1 mstr* wrkr*)
do 
NEW_IP=$(getent hosts $(echo ${EACH} | awk -F. '{print$1}') | awk '{print$1}')
echo "sed -i 's/${FIRST_IP}/${NEW_IP}/g' ${EACH}" >> /tmp/IP_update
done
----
** Review the /tmp/hostname_update file to ensure the sed commands to change the IP addresses are correct: `grep -v suse /tmp/IP_update`
*** Correct any mistakes in the file, then execute the commands in it: `bash /tmp/IP_update`

==== Test that each Worker Node XML file is available through the nginx webserver

NOTE: If the webserver on the Managment Workstation is using a port other than 80, specify that port in the command below, i.e. `curl -s http://kubeadmin:<port>/${EACH}.xml`. The port can be verified by running `sudo podman ps` on the Management Workstation.

----
for EACH in $(awk -F. '{print$1}' .all_nodes)
do 
echo ${EACH}
curl -s http://kubeadmin/${EACH}.xml| egrep "<hostname|ipaddr" | grep -v 127
echo ""
done
----
** Verify each hostname and IP address is correct for each cluster node


[[anchor-40]]
=== AutoYaST install the remaining Dell R640 and R740 servers

NOTE: Perform the following steps on each of the remaining Dell servers, adjusting the IP address and hostname portions of the AutoYaST parameters below

IMPORTANT: The procedure for installing via AutoYaST is slightly different depending on if the target server is configured to boot in (U)EFI mode or Legacy BIOS mode. Be sure to verify the boot mode for a bare-metal server before continuing. Virtual Machines commonly boot in Legacy BIOS mode. For more information, see the SLES15 SP1 AutoYaST guide: https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast

* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the BIOS of the Master Node
* Start the Master Node from DVD or ISO,  Select "Installation" at the DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key. Then, provide the AutoYaST parameters shown below, at the end of the "linuxefi" line (Be sure to insert a space after "splash=silent"). When ready, press Ctrl+x to boot the system.

NOTE: If the webserver on the Managment Workstation is using a port other than 80, specify that port in the command below, i.e. `autoyast=http://kubeadmin:<port>/<node_name>.xml`. The port can be verified by running `sudo podman ps` on the Management Workstation.

*** AutoYaST parameters: `autoyast=http://kubeadmin/<node name>.xml ifcfg=em1=dhcp`
*** If DHCP is not available provide the network configuration parameters in the form of: `ifcfg=em1=<node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain> hostname=<FQDN of node>`

TIP: In some cases the command above will fail to resolve to the correct IP address for the Management Workstation. In those cases, simply use the Management Workstation's cluster network IP address.


[[anchor-50]]
=== Prepare the nodes to be integrated into the CaaS Platform cluster

NOTE: The following commands must be run from the Management Workstation and require a .all_nodes file that contains the fully qualified hostnames of all cluster nodes. 

.Populate the Admin node's known_hosts file with the public keys of the cluster nodes:
----
cat /dev/null > ~/.ssh/known_hosts
for EACH in $(cat ~/autoyast_templates/.all_nodes)
do 
      ssh-keyscan ${EACH} >> ~/.ssh/known_hosts
      ssh-keyscan $(getent hosts ${EACH} | awk '{print$1}') >> ~/.ssh/known_hosts
done
----

.Check the time skew of all nodes to be added to the cluster

* Ensure all nodes respond and the time skew is within 1 second: 
----
rm /tmp/*time-check
for EACH in `cat ~/autoyast_templates/.all_nodes` 
do bash -c "ssh $EACH date > /tmp/$EACH.time-check & " 
done 
date > /tmp/$(hostname -f).time-check 
sleep 1
grep -v suse /tmp/*time-check
rm /tmp/*time-check
----



.Enable passwordless sudo access for the geeko user
* Run the loop below and execute the command that follows it for each node:
----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do ssh $EACH ; done
----
* Execute this command for each node: 
----
sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' > /etc/sudoers.d/01geeko"; exit
----

NOTE: This step also verifies that DNS resolution is configured correctly. If resolution fails, ensure DNS records are correct and that the Management Workstation is using the correct DNS server



.Verify passwordless SSH and sudo capabilities for the geeko user on all nodes
----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do ssh $EACH sudo hostname -f; done
----
** It should return each fully qualified hostname with no additional interaction required
*** If any hosts prompt for a password, resolve the issue with `ssh-copy-id -i /home/geeko/.ssh/id_rsa.pub <hostname>` and retest

[[anchor-60]]
=== Register SLES, CaaS Platform, basesystem, and the containers modules

* Set this variable to the hostname or IP address of the RMT server: `export RMT_HOSTNAME=`
----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do 
echo $EACH 
ssh $EACH sudo SUSEConnect --url http://${RMT_HOSTNAME} && \
ssh $EACH sudo SUSEConnect -p sle-module-containers/15.1/x86_64 && \
ssh $EACH sudo SUSEConnect -p caasp/4.0/x86_64 --url http://${RMT_HOSTNAME}
done
----


.Ensure caasp, SLES, basesystem, and containers are all "Registered"

* Each product should be followed by a line that says "Registered"
----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do 
echo $EACH 
ssh $EACH sudo SUSEConnect -s | egrep -o --color "caasp|SLES|basesystem|containers|server-applications|\"Registered\"" && \
echo"" && read -p "Press Enter for next system"
done
----

==== Ensure swap is not enabled on any of the CaaS Platform hosts

----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do echo $EACH; ssh $EACH grep -v Filename /proc/swaps; echo ""; done
----
** Should return a header line for each node, but nothing else

==== Ensure name resolution is correcly configured on all cluster nodes and the Management Workstation

----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do echo $EACH; ssh $EACH tail -2 /etc/resolv.conf; echo ""; done
echo kubeadmin
tail -2 /etc/resolv.conf
----

[[anchor-65]]
=== Deploy at least one load balancer container

NOTE: The load balancer(s) will present a cluster IP address for API clients to consume

* Follow the README.adoc file to create at least one https://github.com/alexarnoldy/nginx-load-balancer-container[nginx load balancer container] on the Management Workstation and/or other VM or non-cluster system
* The README.adoc file contains a link to the https://github.com/alexarnoldy/opensuse-keepalived-container[openSUSE keepalived container] which maintains the virtual IP address
* Ensure the DNS server that supports the cluster nodes can resolve a FQDN to the API virtual IP address, such as "172.29.145.50	mstr.suse.hpc.local"

==== Ensure all cluster nodes can reach resolve the name of, and reach, the load balancer

----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do echo $EACH; ssh $EACH ping -c 2 mstr.suse.hpc.local; echo ""; done
----

==== Ensure all cluster nodes can reach resolve the name of, and reach, the RMT server

----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do echo $EACH; ssh $EACH ping -c 2 smt.suse.hpc.local; echo ""; done
----

[[anchor-70]]
=== Bootstrap and install the CaaS Platform cluster

NOTE: Many of the steps in this process begin with running an ssh-agent process. While there's little harm in having multiple ssh-agents running at the same time, there's a security advantage to only running the ssh-agent when it's required. This methodology offers the added advantage of ensuring the ssh-agent is running during the procedure when it is required.

NOTE: Perform the following steps from the Management Workstation

////
.For any VM nodes, snapshot before instantiating the cluster
* Create snapshot
----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do echo $EACH; ssh $EACH sudo virsh snapshot-create-as $EACH --name "before forming cluster"; echo ""; done
----
////

* Set the `CONTROL_PLANE` (set to mstr.suse.hpc.local in this guide) and `CLUSTER_NAME` (set to suse-caasp-hpc in this guide) variables and install the CaaS Platform management software and initialize the cluster definition:
----
export CONTROL_PLANE=
export CLUSTER_NAME=
----
----
eval "$(ssh-agent -t 180)"
ssh-add /home/geeko/.ssh/id_rsa
sudo zypper --non-interactive install -t pattern SUSE-CaaSP-Management
cd ~; skuba cluster init --control-plane ${CONTROL_PLANE} ${CLUSTER_NAME} && cd ~/${CLUSTER_NAME}
kill ${SSH_AGENT_PID}
----

NOTE: --control-plane defines the FQDN of the load balancer. The second argument is the name to be given to the new cluster.

////
* Ensure the SSH Agent is running and has the geeko user's RSA key loaded
** `ps -ef | grep ssh-agent`
*** If this doesn't return an ssh-agent running for the geeko user, run the following commands to start ssh-agent and add the Management Workstation's key:
**** `eval "$(ssh-agent)"`
**** `ssh-add /home/geeko/.ssh/id_rsa`
////

* Set the FIRST_MASTER_FQDN (set to mstr1.suse.hpc.local in this guide) and FIRST_MASTER (set to mstr1 in this guide) and bootstrap the cluster:

CAUTION: These procedures utilize the pattern "mstr" to identify master nodes and "wrkr" to identify worker nodes. Make adjustments, as appropriate to match the fully qualified hostnames specified in the .all_nodes file.

----
export FIRST_MASTER_FQDN=$(grep mstr ~/autoyast_templates/.all_nodes | head -1)
export FIRST_MASTER=$(echo ${FIRST_MASTER_FQDN} | awk -F. '{print$1}')
----
----
eval "$(ssh-agent -t 180)"
ssh-add /home/geeko/.ssh/id_rsa
skuba node bootstrap --user geeko --sudo --target ${FIRST_MASTER_FQDN} ${FIRST_MASTER}
kill ${SSH_AGENT_PID}
----

NOTE: This command bootstraps the CaaS Platform cluster (using mstr1.suse.hpc.local in this guide) as the first master node. Internally, Kubernetes will use the host nodename as the Kubernetes cluster nodename (mstr1 in this guide).

==== Join additional Master Nodes to the cluster
* To join all remainging Master Nodes in the ~/autoyast_templates/.all_nodes file, cd into the cluster directory (i.e. ~/suse-hpc-local/), then:

----
## Start by gathering all of the master nodes, less the first one
for MASTER_FQDN in `grep mstr ~/autoyast_templates/.all_nodes | tail -n+2`; do \
eval "$(ssh-agent -t 180)"
ssh-add /home/geeko/.ssh/id_rsa
## This will use the host nodename as the Kubernetes nodename
MASTER=`echo ${MASTER_FQDN} | awk -F. '{print$1}'`; \
skuba node join --role master --user geeko --sudo \
--target ${MASTER_FQDN} ${MASTER}; \
kill ${SSH_AGENT_PID}
done
----

* To join a single Master Node to the cluster, cd into the cluster directory (i.e. ~/suse-hpc-local/), then:

----
eval "$(ssh-agent -t 180)"
cd ~/suse-caasp-hpc/
ssh-add /home/geeko/.ssh/id_rsa
export MASTER_FQDN=
----
----
MASTER=`echo ${MASTER_FQDN} | awk -F. '{print$1}'`; \
skuba node join --role master --user geeko --sudo \
--target ${MASTER_FQDN} ${MASTER}
kill ${SSH_AGENT_PID}
----

IMPORTANT: If any nodes will require additional configuration such as GPU integration, use the command `kubectl cordon <node name>` to prevent work from being assigned to it until it is ready.

==== Join Worker Nodes to the cluster
* To join all remainging Worker Nodes in the ~/autoyast_templates/.all_nodes file, cd into the cluster directory (i.e. ~/suse-hpc-local/), then:

----
for WORKER_FQDN in `grep wrkr ~/autoyast_templates/.all_nodes`; do \
eval "$(ssh-agent -t 180)"
ssh-add /home/geeko/.ssh/id_rsa
WORKER=`echo ${WORKER_FQDN} | awk -F. '{print$1}'`; \
skuba node join --role worker --user geeko --sudo \
--target ${WORKER_FQDN} ${WORKER}; \
kill ${SSH_AGENT_PID}
done
----

* To join a single Worker Node to the cluster, cd into the cluster directory (i.e. ~/suse-hpc-local/), then:
----
eval "$(ssh-agent -t 180)"
ssh-add /home/geeko/.ssh/id_rsa
export WORKER_FQDN=
----
----
WORKER=`echo ${WORKER_FQDN} | awk -F. '{print$1}'`; \
skuba node join --role worker --user geeko --sudo \
--target ${WORKER_FQDN} ${WORKER}
kill ${SSH_AGENT_PID}
----

IMPORTANT: If any nodes will require additional configuration such as GPU integration, use the command `kubectl cordon <node name>` to prevent work from being assigned to those nodes until they are ready.


==== Verify the status of the cluster
* `cd /home/geeko/suse-caasp-hpc`
* `skuba cluster status`

==== Enable the use of kubectl from the Management Workstation
* `echo export KUBECONFIG=/home/geeko/suse-caasp-hpc/admin.conf >> ~/.bashrc`
* `. ~/.bashrc` 
* `kubectl get nodes`

////
.For any VM nodes, snapshot immediately after instantiating the cluster (assumes only the cluster nodes VMs contain the string "suse.hpc.local")
* Create snapshot script from the KVM host:
----
cat <<EOF> /tmp/snap_after_cluster.sh
/bin/bash
sudo virsh list --all | grep "suse.hpc.local" | awk '{print$2}' > /tmp/k8s_nodes
for K8S_NODE in `cat /tmp/k8s_nodes`; do sudo virsh snapshot-create-as \${K8S_NODE} --name "after forming cluster"; done
EOF
----

* scp the script to each node and execute it
----
for EACH in `cat .all_kvm_hosts`; do \
echo $EACH; scp /tmp/snap_after_cluster.sh $EACH:/tmp; \
ssh $EACH /tmp/snap_after_cluster.sh ; echo ""; \
done
----
////



[[anchor-80]]
=== Configure Dex and Gangway

==== Create the ClusterRoleBinding for the admins group
* `kubectl create clusterrolebinding ldap-admin-access --clusterrole=cluster-admin --group=admins`
* Edit the configmap: `kubectl --namespace=kube-system edit configmap oidc-dex-config`
* Restart Dex and Gangway pods:
----
kubectl --namespace=kube-system delete pod -l app=oidc-dex
kubectl --namespace=kube-system delete pod -l app=oidc-gangway
----
* Verify the new pods have started correctly: 
----
kubectl get pods -n kube-system -l app=oidc-dex -o wide 
kubectl get pods -n kube-system -l app=oidc-gangway -o wide 
----

==== Add the Dex self-signed cert to the Admin node:
* Get the cert from the dex pod: `kubectl exec -it -n kube-system $(kubectl get pod -n kube-system -l app=oidc-dex -o name | head -1) cat /etc/dex/pki/ca.crt > /tmp/mstr.suse.hpc.local:32000-ca.crt`
* Move the cert into place: `sudo mv /tmp/mstr.suse.hpc.local:32000-ca.crt /etc/pki/trust/anchors/`
* Update the certs: `sudo update-ca-certificates`

==== Test authentication:
* `skuba auth login -s https://mstr.suse.hpc.local:32000`
** Use the uid from oidc-dex-config configmap, i.e. if the user "suse" was specified in `bindDN: uid=suse` and the password "suse1234" was specified in `bindPW: suse1234`
* Remove the kubeconf.txt file, if the login was successful: `rm kubeconf.txt`
* Use the Chromium browser to test logging into Gangway at https://mstr.suse.hpc.local:32001
** Log in with a same user (rather than email address, as it shows) and password, as was used with Dex
** Try using Incognito Mode if cookie errors prevent logging in

[[anchor-90]]
=== Configure the Ceph storage class

NOTE: The following set of commands from the *SES Administrative Workstation*.

TIP: Use the command `ceph -s | grep osds` to find the number of OSDS in the ceph cluster. Use that information with the ceph osd calculator (https://ceph.com/pgcalc/) to get the number of Placement Groups "Suggested PG Count" that will be appropriate for the pool (CephPGSize)

.If needed, create the SES (Ceph) storage pool and enable RBD support
----
export CephPool="hpc-rbd-pool"
export CephPGSize="64"
ceph osd pool create ${CephPool} ${CephPGSize}
ceph osd pool ls
ceph osd pool application enable ${CephPool} rbd
ceph osd pool application get ${CephPool}
----

TIP: Use the command `ceph auth ls | grep hpc-rbd-user` to ensure the user hasn't already been created. If it has, skip this step and continue onto "Gather the keys for the SES admin and data-hub-demo users"

.If needed, Create the user that will manage the pool

----
export CephUser="hpc-rbd-user" 
export CephPool="hpc-rbd-pool"
sudo ceph auth get-or-create client.${CephUser} mon 'allow r' osd "allow class-read object_prefix rbd_children, allow rwx pool=${CephPool}" -o /etc/ceph/ceph.client.${CephUser}.keyring
----


.Gather the keys for the SES admin and data-hub-demo users
----
ceph auth ls  | egrep -A1 "${CephUser}|admin"
----
* Example ouput:
----
client.admin
        key: AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==
----

.Encode each of the keys (admin key used as an example):
----
echo -n "AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==" | base64
----
* Example ouput:
`QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==`


NOTE: The next commands should be run on the *CaaS Platform Management Workstation*

.Create the ceph-admin-secret
* Set this variable with the base64 encoded admin key: `ADMIN_KEY=""`
** For example: ADMIN_KEY="QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ=="
----
cat <<EOF> ceph-secret-admin.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
  namespace: default
type: "kubernetes.io/rbd"
data:
  key: $ADMIN_KEY
EOF
----

* Set this variable with the base64 encoded hpc-rbd-user key: `CEPH_USER_KEY=""`
** For example: CEPH_USER_KEY="QVFEaVJNdGR6K3dYTlJBQUhhTmRqS1c1eTl5MUd2VWkyZjhnS2c9PQ=="
----
cat <<EOF> ceph-secret-hpc-rbd-user.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-hpc-rbd-user
  namespace: default
type: "kubernetes.io/rbd"
data:
  key: $CEPH_USER_KEY
EOF
----

.Apply the two Kubernetes secrets:
* `kubectl apply -n data-hub -f ceph-secret-admin.yaml`
* `kubectl apply -n data-hub -f ceph-secret-hpc-rbd-user.yaml`

.Create the SES6 storage class:
* Set this variable to the ssh credentials for the SES Administrative Workstation: `SSH_SES_ADMIN=""`
** For example: SSH_SES_ADMIN=root@admin.suse.hpc.local
* Set the MONITORS variable to include the IP addresses and ports for the SES monitor nodes:
----
MONITORS=`ssh $SSH_SES_ADMIN "grep mon_host /etc/ceph/ceph.conf" | awk -F" = " '{print$2}' | sed 's/\, /:6789\,/g' | sed 's/$/:6789/'`
echo $MONITORS
----
** The output should be similar to: `172.29.147.41:6789,172.29.147.40:6789,172.29.147.42:6789`

* Create the SES6 storage class
----
cat <<EOF> hpc-rbd-sc.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ses-rbd-sc
  annotations:
     storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: $MONITORS
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: default
  userId: hpc-rbd-user
  userSecretName: ceph-secret-hpc-rbd-user
  pool: hpc-rbd-pool
EOF
----

* Apply the kubernetes storage class:
`kubectl apply -f ses-rbd-sc.yaml`
* Verify the SES6 storage class is the default:
`kubectl get storageclass`

.Create a test PVC and ensure it can be bound:
----
cat <<EOF> test-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
EOF
----
* Apply the kubernetes PVC:
`kubectl apply -f test-pvc.yaml`
* In less than one minute, the PVC should show that is "Bound" to "VOLUME":
`kubectl get pvc`
* Delete the PVC after it has shown to be Bound:
`kubectl delete -f test-pvc.yaml`



[[anchor-100]]
=== Enable privilege escalation on CaaS Platform

==== Edit the pod security policy
* `kubectl edit psp suse.caasp.psp.unprivileged`
*** Add the following elements as part of the `spec:`
**** Note: If `allowPrivilegeEscalation` is already defined, ensure it is set to true and add the allowedCapabilities
------
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - SYS_PTRACE
  - NET_ADMIN
------



[[anchor-110]]
=== Enable GPU availability for CaaS Platform

.Validate the nodes that are equipped with Nvidia GPUs
----
for EACH in `cat ~/autoyast_templates/.all_nodes`; do echo -n $EACH": "; ssh $EACH sudo lspci | grep -i nvidia; echo ""; done
----

.Ensure the GPU equipped Worker Nodes are cordoned (marked with Ready,SchedulingDisabled): `kubectl get nodes`
* If any nodes need to be cordoned, use th command: `kubectl cordon <node name>`

### Prepare the Nvidia GPU equipped Worker Node

NOTE: Complete the following steps for *each Nvidia GPU equipped node* before continuing to the next section (Process to install nvidia-container-toolkit on Nvidia GPU equipped nodes)

* Verify the model of Nvidia GPU: `sudo lspci | grep -i nvidia`
** Check against: https://developer.nvidia.com/cuda-gpus to ensure the GPU is CUDA compatible

* Install the appropriate kernel header files for the kernel version and variant
** `uname -r`
*** Output is in the form of <version>-<variant>, i.e. for "4.12.14-197.26-default" the version would be "4.12.14-197.26" and the version would be "default"
** Set these variables:
----
export VERSION=
export VARIANT=
----
** Install the  packages: `sudo zypper --non-interactive install kernel-${VARIANT}-devel=${VERSION}`


* Install the Cuda toolkit (currently specific to SLES 15 SP1):
** Update the needed repositories:
----
sudo zypper addrepo http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/cuda-sles15.repo
sudo SUSEConnect --product PackageHub/15.1/x86_64
sudo SUSEConnect --product sle-module-desktop-applications/15.1/x86_64
----
** Refresh the repositories and install cuda:
----
sudo zypper refresh     
sleep 5
sudo zypper --non-interactive install cuda
----

* Verify which packages and versions of the Cuda toolkit were installed `sudo zypper search --installed cuda`

* When the driver is correctly loaded it will show version information in: `cat /proc/driver/nvidia/version`
** If the driver hasn't loaded, reboot the node and check again

* Check that you can access the GPU:
----
sudo usermod -G video -a geeko
sudo usermod -G video -a root
sudo su - geeko
nvidia-smi
----
** Should get an output that contains:
----
NVIDIA-SMI XXX.YY Driver Version: XXX.YY CUDA Version: XX.Y
. . . .
No running processes found
----

* Install the Nvidia libnvidia-container:
----
wget https://github.com/NVIDIA/libnvidia-container/releases/download/v1.0.0/libnvidia-container_1.0.0_x86_64.tar.xz
tar xJf libnvidia-container_1.0.0_x86_64.tar.xz
sudo cp libnvidia-container_1.0.0/usr/local/bin/nvidia-container-cli /usr/bin
sudo cp libnvidia-container_1.0.0/usr/local/lib/libnvidia-container.so* /usr/lib64
----
* Verify functionality of the nvidia-container-cli utility: `nvidia-container-cli info`
** Should get an output that contains:
----
NVRM version:   XXX.YY                                                          
CUDA version:   XX.Y  
Model:          X
Brand:          Y
----

### Process to install nvidia-container-toolkit on Nvidia GPU equipped nodes

NOTE: Start the process on the *CaaS Platform Administrative Workstation*

* Download the required package via a CentOS container:
----
sudo zypper --non-interactive install podman
sudo podman run --rm -ti -v$PWD:/var/tmp centos:7
DIST=$(. /etc/os-release; echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$DIST/nvidia-container-runtime.repo |    tee /etc/yum.repos.d/nvidia-container-runtime.repo
yum install --downloadonly nvidia-container-runtime-hook        
cp /var/cache/yum/x86_64/7/nvidia-container-runtime/packages/nvidia-container-toolkit-1.0.5-2.x86_64.rpm /var/tmp
exit
----

* Edit  the `etc/nvidia-container-runtime/config.toml` file to uncomment or insert the line: `user = "root:video"`

* Create the unrpm script from: https://github.com/openSUSE/obs-build/blob/master/unrpm
* Unpack the rpm: `bash unrpm nvidia-container-toolkit-1.0.5-2.x86_64.rpm`
* If running this on another node (i.e. the Administrative Workstation), SCP the files to the GPU Worker Node:
** Set the WORKER variable to the FQDN of one of the GPU equipped Worker Nodes and SCP the files to that Worker Node:
----
export WORKER=
----

----
scp -r etc/ $WORKER:~/
scp -r usr/ $WORKER:~/
----
*** Repeat for all remainin GPU equipped Worker Nodes

==== Perform the following steps on each GPU equipped Worker Node:

* Copy the Nvidia Container Toolkit into place:
----
sudo mkdir -p /etc/nvidia-container-runtime/
sudo mkdir -p /usr/libexec/oci/hooks.d/
sudo mkdir -p /usr/share/licenses/nvidia-container-toolkit-1.0.5/
----

----
sudo cp etc/nvidia-container-runtime/config.toml /etc/nvidia-container-runtime/config.toml
sudo cp usr/bin/nvidia-container-toolkit /usr/bin/nvidia-container-toolkit
sudo cp usr/share/containers/oci/hooks.d/oci-nvidia-hook.json /usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
sudo cp usr/libexec/oci/hooks.d/oci-nvidia-hook /usr/libexec/oci/hooks.d/oci-nvidia-hook
sudo cp usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE /usr/share/licenses/nvidia-container-toolkit-1.0.5/LICENSE
----

* Update the metadata of the Nvidia device files:
----
sudo chmod 0666 /dev/nvidia*
sudo chown root:video /dev/nvidia*
----

* (Optional) Install Podman on the Worker Node and test that a container can access the GPU: 
----
sudo zypper --non-interactive install podman
sudo podman run --rm --net=host nvidia/cuda nvidia-smi
----
** Should get an output that contains:
----
NVIDIA-SMI XXX.YY Driver Version: XXX.YY CUDA Version: XX.Y
. . . .
No running processes found
----

==== Finish the process from the Administrative Workstation
* Install the Nvidia Kubernetes device plugin 
* `kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml`

* Set this variable to the CaaSP node name (not the FQDN) for the next several commands, then repeat for each GPU equipped worker node: `export WORKER=`

* Ensure the correct number of GPUs are recognized on the worker node: `kubectl describe node $WORKER | egrep "gpu|Unschedulable"`
** Output should include three lines beginning with `nvidia.com/gpu`. The first two should match the number of GPUs on the node. The last line should show quanties zero

NOTE: If the previous command also showed `Unschedulable` as `true`, uncordon the node before continuing: `kubectl uncordon $WORKER`

.Ensure that CaaS Platform can run a GPU enabled pod on the node:

* Set this variable to the number of GPUs on this node: `export GPUS=`
* Create the cuda-vector-add.yaml file:
----
cat <<EOF> cuda-vector-add.yaml
apiVersion: v1                                                                  
kind: Pod                                                                       
metadata:                                                                       
  name: cuda-vector-add                                                         
spec:                                                                           
  restartPolicy: OnFailure                                                      
  nodeSelector:
    kubernetes.io/hostname: $WORKER
  containers:                                                                   
    - name: cuda-vector-add                                                     
      # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile
      image: "k8s.gcr.io/cuda-vector-add:v0.1"                                  
      resources:                                                                
        limits:                                                                 
          nvidia.com/gpu: $GPUS
EOF
----

* Apply the pod creation file and review the pod's logs and node assignment: `kubectl apply -f cuda-vector-add.yaml` 
* Check the pod until the pod shows a status of "Completed": `kubectl get pods -o wide | grep cuda-vector-add`
* Review the log output of the container: `kubectl logs cuda-vector-add` 
** Output should include phrases such as `CUDA kernel launch` and `Test PASSED`, as well as show that the pod ran on the correct node
* Remove the pod: `kubectl delete -f cuda-vector-add.yaml`

### The deployment and integration are complete!


[[anchor-120]]
=== Troubleshooting 

==== Troubleshoot a failed bootstrap
* ssh to master and `sudo less /var/log/messages` 
* Search for kub
* Follow the progression of the skuba command and kubeadm
** Generally skuba will install the packages, then launch kubeadm
** kubeadm will set up the K8s components
** If the failure occurs after kubeadm takes over try to replicate the failure:
*** scp kubeadm-init.conf from the cluster directory (suse-caasp-hpc in this doc) to /tmp on the master node
*** Run the `kubeadm init` command that is in /var/log/messages
*** kubeadm should give reasonably actionable error messages






// vim: set syntax=asciidoc:
